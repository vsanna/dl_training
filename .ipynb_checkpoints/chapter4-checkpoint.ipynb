{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.104416666667, 0.1028\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# 四章 NNの学習\n",
    "- NNの学習 = 訓練データから最適な重みパラメータの値を自動で獲得すること\n",
    "- NNが学習を行えるようにするため、損失関数を導入する\n",
    "    - この値が最も小さくなる重みパラメータを探し出すのが学習の目的\n",
    "    - まずは勾配法を導入する\n",
    "- NNはデータから学習できることが特徴\n",
    "- 機械学習はデータが中心\n",
    "    - ex: パーセプトロン\n",
    "    - データをまず用意する必要がある\n",
    "    - また、推測を介さず、データからのみストーリーを語る    \n",
    "- たとえば、ゼロから「5」を認識するアルゴリズムをひねり出すより、画像から特徴量を抽出して、そのパターンを学習する方法がある\n",
    "    - 特徴量とは、入力データから、本質的なデータを的確に抽出できるように設計された「変換器」を指す\n",
    "    - 画像の特徴量は一般的にベクトルで記述される\n",
    "        - コンピュータビジョンで有名な特徴量としては、SIFT, SURF, HOGなどがある\n",
    "    - 画像データを変換器　= 特徴量によってベクトルに変換し、そのベクトルに対して「識別器」で学習させる\n",
    "        - [ ] 識別器とは\n",
    "            - SVN, KNN等があるらしい\n",
    "- この「特徴量」については人間が設計する必要がある\n",
    "    - 問題に応じて特徴量 = 変換器を使い分ける必要がある\n",
    "    - ココには人の手が介在する\n",
    "- ここまでの推測の種類の生理\n",
    "    1. 入力 -> 人間の考えたアルゴリズム -> 出力\n",
    "    2. 入力 -> 人間の考えた特徴量 -> 機械学習 -> 出力\n",
    "    3. 入力 -> NN(deep learning) -> 出力\n",
    "- よって、NN(DL)はend-to-end-machine-learningと呼ばれることがある\n",
    "\n",
    "## データの取扱について\n",
    "- 機械学習の問題では、訓練データとテストデータの2つにデータを分けて学習やじっけんを行なう\n",
    "    1. 訓練データで適切なパラメータを探索\n",
    "    2. テストデータでモデルの実力を評価\n",
    "- 汎化能力を正しく評価するためにデータを2つに分離する必要がある\n",
    "    - 汎化能力 = まだ見ぬデータへの推測力\n",
    "    - データセットが偏っていると、特定のデータセットにうまく対応できても、他のには対応できなくなると行った状況に陥る\n",
    "    - これを過学習 = overfittingという\n",
    "\n",
    "## 損失関数\n",
    "- NN性能の「悪さ」を示す指標\n",
    "- 通常は二乗和誤差や、交差エントロピー誤差などが用いられる\n",
    "\n",
    "### 二乗和誤差\n",
    "- E = 0.5 * ∑(yk - tk)^2 ... kは添字\n",
    "    - y: NNの出力, tは教師データ\n",
    "- 教師データにおいて、正解データを1, それ以外を0と置く表記をone-hot表現という\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# chapter3の例\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # softmax関数の出力\n",
    "t = [    0,       0,    1,     0,        0,   0,     0,    0,     0,    0] # 教師データ\n",
    "\n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "    \n",
    "# 「２」を正解とする\n",
    "t = [    0,       0,    1,     0,        0,   0,     0,    0,     0,    0] \n",
    "\n",
    "# ex1: 2の確率が最も高いと弾き出すモデルの出力の例\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t)) # 0.097...\n",
    "\n",
    "# ex2: 7の確率が最も高いと弾き出すモデルの出力の例\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t)) # 0.597...\n",
    "\n",
    "# 確かにこの損失関数は誤っているex2の方でおおきな値を返している\n",
    "\n",
    "'''\n",
    "### 交差エントロピー誤差\n",
    "- E = - ∑(tk * log(yk)) ... kは添字\n",
    "- one-hot表現だとすると、tk=1、つまり正解ラベルのときだけ計算する\n",
    "- また、softmaxの出力値で正解の確率を仮に1と出していた時、損失は0になる = 優秀であると判断できる\n",
    "'''\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7 # 微小な値\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "# np.log(0)を計算してしまうとnp.log(0)が-infとなり、エラーが起きてしまうため\n",
    "\n",
    "\n",
    "# 「2」を正解とする\n",
    "t = [    0,       0,    1,     0,        0,   0,     0,    0,     0,    0] \n",
    "\n",
    "# ex1: 2の確率が最も高いと弾き出すモデルの出力の例\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t)) # 0.510825457099\n",
    "\n",
    "# ex2: 7の確率が最も高いと弾き出すモデルの出力の例\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t)) # 2.3025840929945458\n",
    "\n",
    "\n",
    "'''\n",
    "## ミニバッチ学習\n",
    "- 全ての訓練データに対して損失関数を求め、その値をできるだけ小さくするようなパラメータを探し出す必要がある\n",
    "- よって、先程までの一つの訓練データの損失関数を求める式の更に総和を求める\n",
    "    - E = -(1/N) * ∑(∑(tnk * logynk)) ... n, kは添字\n",
    "- しかし、MNISTのデータセットであっても6万件あり、bigdataにもなればかなりのデータ量になる\n",
    "    - 全てのデータを対象に損失関数を計算するのは非現実的\n",
    "- そこで、訓練データからある枚数だけを選び出し、その塊ごとに学習を行なう\n",
    "    - この選びだしたものをミニバッチ = 小さな塊という\n",
    "    - [ ] 選び出す数 = batch_sizeの適切な決め方は？\n",
    "'''\n",
    "\n",
    "import sys, os\n",
    "sys.path.append('./deep-learning-from-scratch')\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_labeｌ=True)\n",
    "\n",
    "train_size = x_train.shape[0] # 60000\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "# ex: array([25286, 10861, 55404, 16791, 26595, 34692, 39286,  9747, 15529, 18955])\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        # reshape: n * m行列を、n' * m'行列に変形してくれる。ただし、要素数がおなじになるような変形に限る\n",
    "        # つまりここでは1次元配列を1 * nの2次元行列に変更している [1,2] => [[1. 2]]\n",
    "        t = t.reshape(1, t.size) \n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "        batch_size = y.shape[0] \n",
    "        return -np.sum(t * np.log(y)) / batch_size\n",
    "    \n",
    "\n",
    "'''\n",
    "### 補足\n",
    "- なぜ損失関数を設けるのか\n",
    "    - モデルの精度をパラメータで微分しても大きく変動しない => どの方向のパラメーターを動かせばいいのかわからない\n",
    "- なぜNNではステップ関数ではなくsigmoidのような非線形関数を用いるのか\n",
    "    - step関数の微分は大体の箇所で0なので、損失関数(実際は定数 - 活性化関数の返り値なので、活性化関数の微分とほぼ同形)の微分も大体０になってしまう\n",
    "'''\n",
    "\n",
    "'''\n",
    "## 数値微分 Numerical Gradient\n",
    "### 数値微分\n",
    "'''\n",
    "# 数値微分を実装する\n",
    "# 中央微分にすることで丸め誤差を減らせるらしい\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# 数値微分の例\n",
    "def func_1(x):\n",
    "    return 0.01 * x ** 2 + 0.1 * x    \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = func_1(x)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "# plt.plot(x, y)\n",
    "# plt.show()\n",
    "\n",
    "def targent_line(f, x):\n",
    "    a = numerical_diff(f, x) # 傾き\n",
    "    def line(t):\n",
    "        return a * t - a * x + f(x)\n",
    "        \n",
    "    return line\n",
    "       \n",
    "        \n",
    "tl = targent_line(func_1, 5)\n",
    "y2 = tl(x)\n",
    "# plt.plot(x, y2)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "'''\n",
    "### 偏微分\n",
    "- 変数が２つの関数の実装\n",
    "'''\n",
    "\n",
    "def func_2(x):\n",
    "    return x[0] ** 2 + x[1] ** 2\n",
    "    # or return np.sum(x**2)\n",
    "\n",
    "# たとえばx0 = 3, x1 = 4のときのx０に対する偏微分\n",
    "def func_2_tmp1(x0):\n",
    "    return x0 ** 2.0 + 4.0 ** 2.0\n",
    "\n",
    "# print(numerical_diff(func_2_tmp1, 3.0))\n",
    "\n",
    "\n",
    "# たとえばx0=3, x1=4のときのx1に対する偏微分\n",
    "def func_2_tmp2(x1):\n",
    "    return 3.0 ** 2.0 + x1 ** 2.0\n",
    "\n",
    "# print(numerical_diff(func_2_tmp2, 4.0))\n",
    "\n",
    "'''\n",
    "## 勾配\n",
    "- x0, x1の偏微分をまとめて計算したい\n",
    "- 勾配 = 全ての変数についての偏微分をベクトルとしてまとめたもの\n",
    "    - p105\n",
    "    - 勾配が示す方向は関数の値を最も減らす方向 = 局値を示す\n",
    "'''\n",
    "\n",
    "# ある座標における勾配(ベクトル)を返す\n",
    "# つまり数値微分\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-10\n",
    "    grad = np.zeros_like(x) # xと同じ形状のゼロ行列を生成\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2)  / (2 * h)\n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        \n",
    "    return grad\n",
    "\n",
    "# 試しに。\n",
    "f = lambda x: x[0] ** 2 + x[1] ** 2\n",
    "x = np.array([1,2])\n",
    "numerical_gradient(f, x)\n",
    "\n",
    "f = lambda x: x**2\n",
    "x = np.array([2])\n",
    "numerical_gradient(f, x)\n",
    "\n",
    "'''\n",
    "### 勾配法\n",
    "- 機械学習の最適なパラメータ探索では損失関数が最小値を撮るようなパラメータを探したいというのが目的\n",
    "- 勾配は各地点において、関数の値を最も減らす方向を示すという特徴を利用する\n",
    "    - とはいっても、勾配の先が真に関数の最小値なのかは担保されない\n",
    "        - 極小値や鞍点の場合も勾配は0になる\n",
    "        - [ ] 鞍点とは\n",
    "        - 関数が複雑で歪な形をしていると、平らな土地に入り込み、プラトーと呼ばれる学習が進まない停滞期に陥ることがある\n",
    "- 具体的には、ある関数の極小値を求めたい時、\n",
    "    1. ある点をスタート地点として任意に定め\n",
    "    2. そこから勾配を算出して、勾配の方向に学習率分だけ歩を進め、\n",
    "    3. さらに2を任意の一定回数繰り返す、ということをくり返す\n",
    "- 勾配法（gradient method）の式\n",
    "    - x0 := x0 - μ * (δf / δx0)\n",
    "        - μは学習率と呼ばれ、一回の学習でどれだけ学習するか = 歩を進めるかを決める\n",
    "        - 一般に大きすぎても小さすぎてもいけない\n",
    "        - 経験値的に0.01 - 0.001当たりをとるが、適切かどうかは適宜確認する\n",
    "    - 特にNNでよく使われる\n",
    "- 下記実例でもあげているが、learning rateをどう設定するか、間違っていないかどうかをどう確かめればいいのか\n",
    "- ハイパーパラメーター\n",
    "    - 学習率など。\n",
    "    - 重みなどのパラメーターは学習によって勝手に取得されるが、学習率は人の手によって設定する必要がある\n",
    "'''\n",
    "\n",
    "def gradient_descent(f, init_x, lr = 0.01, step_num = 100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x = x - lr * grad\n",
    "        \n",
    "    return x\n",
    "\n",
    "# 試しに。\n",
    "f = lambda x: x[0] ** 2 + x[1] ** 2\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(f, init_x, lr = 0.01, step_num=100) # array([ -6.11110734e-10,   8.14814083e-10])\n",
    "# たとえば、lr=0.001とかにすると、[-2.45570027,  3.27426721]にまでしか到達しない。\n",
    "\n",
    "\n",
    "'''\n",
    "## NNの勾配\n",
    "- NNにおける勾配とは、重みパラメーターを変数とした損失関数の勾配のこと\n",
    "- 損失関数をL, 重みをWで表現する時、δL/δWを勾配とする\n",
    "'''\n",
    "\n",
    "# 実際のNNで実装する\n",
    "# 3章で実装したものを再度呼び出している\n",
    "import sys, os\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "# このnumerical_gradientは\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3) # ガウス分布で2*3の行列を初期化して取得\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(x)\n",
    "        return  cross_entropy_error(y, t)\n",
    "    \n",
    "net = simpleNet()\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "np.argmax(p)\n",
    "\n",
    "t = np.array([0, 0, 1]) # 正解ラベル\n",
    "net.loss(x, t)\n",
    "\n",
    "f = lambda w: net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "'''\n",
    "## 学習アルゴリズムの実装\n",
    "- 一旦まとめる\n",
    "    1. 前提\n",
    "        - NNは適応可能な重みとバイアスがあり、この思おみトバイアスを訓練データに適応するように調整することを学習と呼ぶ.4stepある\n",
    "    2. step1 ミニバッチ\n",
    "        - 訓練データの中からランダムに一部のデータを選び出し、そのミニバッチに対する損失関数を小さくすることを目的とする\n",
    "    3. step2 勾配の算出\n",
    "        - スタート地点となるパラメーター(W, b)は適当におく\n",
    "        - ミニバッチの損失関数を減らすために各重みパラメーターの勾配(各wに対する偏微分. ここでは定数が出て来る)を求める\n",
    "        - 勾配は損失関数を最も減らす方向を示す\n",
    "    4. step3 パラメタの更新\n",
    "        - 重みパラメータを勾配方向に微小量 = 学習率だけ更新する\n",
    "    5. step4 繰り返す\n",
    "        - step_numだけ繰り返す\n",
    "- ミニバッチでの勾配降下法は通称確率的勾配降下法と呼ばれる。Stochastic Gradient Descentの頭文字をとり、通常SGDという名前の関数で実装される\n",
    "- [ ] hidden_sizeはどう定めるのか\n",
    "- gradの理解\n",
    "    - N個のレコードとN個の正解データから1個の損失関数を生成している(ref: L = -∑(1ペアの損失差分) )\n",
    "    - これで生成された損失関数を元にWのgradを生成し、Wの更新をしている\n",
    "    - (よく考えれば当たり前だが)batch処理をすれば、各イテレーション毎の損失関数は(とても似ているとは思われるが)毎回異なる形をしている\n",
    "'''\n",
    "# いよいよMNISTを実装する\n",
    "\n",
    "class TwoLayerNet:\n",
    "    # input_size: 1レコードがもつニューロンの個数\n",
    "    # hidden_size: 中間層のニューロンの個数\n",
    "    # output_size: 出力層のニューロンの個数 = 分類問題であれば分類のカテゴリ数\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) # 784 * 100の行列\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        # バイアスを0で実装する\n",
    "        self.params['b1'] = np.zeros(hidden_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        z2 = softmax(a2)\n",
    "        return z2\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accracy\n",
    "    \n",
    "    # 損失関数に対する現在位置での勾配を返す\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t) # self.lossが返す、cross_entropy_error(y, t)はWにのみ依存する関数\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    \n",
    "# example\n",
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "x = np.random.rand(100, 784) # 100 * 784のダミーデータ\n",
    "t = np.random.rand(100, 10)    # 100 * 10のダミーデータ\n",
    "grads = net.numerical_gradient(x, t)\n",
    "\n",
    "# MNISTでの実用\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "train_loss_list = []\n",
    "\n",
    "# hyper parameter\n",
    "iters_num = 100 # 1万回繰り返す\n",
    "train_size = x_train.shape[0] # 60000個の訓練データ. shape[0]はレコード数を返す\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "nw = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# N個のレコードとN個の正解データで1個のloss_W関数をまず作っている. これがself.loss\n",
    "# L(W) = -∑(l(w)) ... l(w)は1個のレコードと正解データから作られる\n",
    "# for i in range(iters_num):\n",
    "#     # ミニバッチの取得\n",
    "#     batch_mask = np.random.choice(train_size, batch_size)\n",
    "#     x_batch = x_train[batch_mask]\n",
    "#     t_batch = t_train[batch_mask]\n",
    "    \n",
    "#     # 勾配の計算(損失関数L(W)の生成 => 数値微分)\n",
    "#     grad = nw.numerical_gradient(x_batch, t_batch)\n",
    "#     # (δL/δw1, δL/δw2, ...)と、偏微分の結果となる定数がベクトルとして出力される\n",
    "#     # grad = nw.gradient(x_batch, t_batch) # 後ほど高速な処理方法を導入\n",
    "#     # print(grad)\n",
    "    \n",
    "#     # パラメータの更新\n",
    "#     for key in ('W1', 'W2', 'b1', 'b2'):\n",
    "#         nw.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "#     # 学習経過の記録\n",
    "#     # 損失関数の値 = 誤差の大きさが徐々に小さくなっていくことを確認できる\n",
    "#     loss = nw.loss(x_batch, t_batch)\n",
    "#     train_loss_list.append(loss)\n",
    "    \n",
    "# print('train', train_loss_list)\n",
    "\n",
    "\n",
    "'''\n",
    "## モデルの評価\n",
    "- 訓練データでモデルを作成したら、それをテストデータで試す\n",
    "    - 訓練データのx, tは「入力値と正解データ（教師データ）」\n",
    "    - testデータのx, tは「入力値と、精度確認データ」\n",
    "- エポック単位で精度を測る\n",
    "    - エポック = 訓練データを全て使い切ったときの回数に対応する\n",
    "    - たとえば、1万個の訓練データに対して100個のミニバッチで学習する場合、100回勾配降下法を繰り返したら全ての訓練データを見たことになる\n",
    "    - よって、この場合、１エポック= 100回となる\n",
    "    - なので、100階ごとに精度を出す\n",
    "'''\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1エポック当たりの繰り返し数\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "train_size = x_train.shape[0] # 60000\n",
    "iters_num = 10000 # 1万回繰り返す\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "nw = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # ミニバッチの取得\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配の計算(損失関数L(W)の生成 => 数値微分)\n",
    "    grad = nw.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # パラメータの更新\n",
    "    for key in ('W1', 'W2', 'b1', 'b2'):\n",
    "        nw.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 学習経過の記録\n",
    "    # 損失関数の値 = 誤差の大きさが徐々に小さくなっていくことを確認できる\n",
    "    loss = nw.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = nw.accracy(x_train, t_train)\n",
    "        test_acc = nw.accracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "        \n",
    "        \n",
    "plot.plot([0,1,2,3,4,5,6,7,8], test_acc_list) # こんな使い方できるの...?\n",
    "\n",
    "x = np.arange(0, 16, 2) # [0, 2, 4, ... 16]\n",
    "y = np.array(test_acc_list)[x]\n",
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
