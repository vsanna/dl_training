{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# 四章 NNの学習\n",
    "- NNの学習 = 訓練データから最適な重みパラメータの値を自動で獲得すること\n",
    "- NNが学習を行えるようにするため、損失関数を導入する\n",
    "    - この値が最も小さくなる重みパラメータを探し出すのが学習の目的\n",
    "    - まずは勾配法を導入する\n",
    "- NNはデータから学習できることが特徴\n",
    "- 機械学習はデータが中心\n",
    "    - ex: パーセプトロン\n",
    "    - データをまず用意する必要がある\n",
    "    - また、推測を介さず、データからのみストーリーを語る    \n",
    "- たとえば、ゼロから「5」を認識するアルゴリズムをひねり出すより、画像から特徴量を抽出して、そのパターンを学習する方法がある\n",
    "    - 特徴量とは、入力データから、本質的なデータを的確に抽出できるように設計された「変換器」を指す\n",
    "    - 画像の特徴量は一般的にベクトルで記述される\n",
    "        - コンピュータビジョンで有名な特徴量としては、SIFT, SURF, HOGなどがある\n",
    "    - 画像データを変換器　= 特徴量によってベクトルに変換し、そのベクトルに対して「識別器」で学習させる\n",
    "        - [ ] 識別器とは\n",
    "            - SVN, KNN等があるらしい\n",
    "- この「特徴量」については人間が設計する必要がある\n",
    "    - 問題に応じて特徴量 = 変換器を使い分ける必要がある\n",
    "    - ココには人の手が介在する\n",
    "- ここまでの推測の種類の生理\n",
    "    1. 入力 -> 人間の考えたアルゴリズム -> 出力\n",
    "    2. 入力 -> 人間の考えた特徴量 -> 機械学習 -> 出力\n",
    "    3. 入力 -> NN(deep learning) -> 出力\n",
    "- よって、NN(DL)はend-to-end-machine-learningと呼ばれることがある\n",
    "\n",
    "## データの取扱について\n",
    "- 機械学習の問題では、訓練データとテストデータの2つにデータを分けて学習やじっけんを行なう\n",
    "    1. 訓練データで適切なパラメータを探索\n",
    "    2. テストデータでモデルの実力を評価\n",
    "- 汎化能力を正しく評価するためにデータを2つに分離する必要がある\n",
    "    - 汎化能力 = まだ見ぬデータへの推測力\n",
    "    - データセットが偏っていると、特定のデータセットにうまく対応できても、他のには対応できなくなると行った状況に陥る\n",
    "    - これを過学習 = overfittingという\n",
    "\n",
    "## 損失関数\n",
    "- NN性能の悪さを示す指標\n",
    "- 通常は二乗和誤差や、交差エントロピー誤差などが用いられる\n",
    "- 二乗和誤差\n",
    "    - E = 0.5 * ∑(yk - tk)^2 ... kは添字\n",
    "    - y: NNの出力, tは教師データ\n",
    "- 教師データにおいて、正解データを1, それ以外を0と置く表記をone-hot表現という\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# chapter3の例\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # softmax関数の出力\n",
    "t = [    0,       0,    1,     0,        0,   0,     0,    0,     0,    0] # 教師データ\n",
    "\n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "    \n",
    "# 「２」を正解とする\n",
    "t = [    0,       0,    1,     0,        0,   0,     0,    0,     0,    0] \n",
    "\n",
    "# ex1: 2の確率が最も高いと弾き出すモデルの出力の例\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t)) # 0.097...\n",
    "\n",
    "# ex2: 7の確率が最も高いと弾き出すモデルの出力の例\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t)) # 0.597...\n",
    "\n",
    "# 確かにこの損失関数は誤っているex2の方でおおきな値を返している\n",
    "\n",
    "'''\n",
    "- 交差エントロピー誤差\n",
    "    - E = - ∑(tk * log(yk)) ... kは添字\n",
    "    - one-hot表現だとすると、tk=1、つまり正解ラベルのときだけ計算する\n",
    "    - また、softmaxの出力値で正解の確率を仮に1と出していた時、損失は0になる = 優秀であると判断できる\n",
    "'''\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7 # 微小な値\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "# np.log(0)を計算してしまうとnp.log(0)が-infとなり、エラーが起きてしまうため\n",
    "\n",
    "\n",
    "# 「2」を正解とする\n",
    "t = [    0,       0,    1,     0,        0,   0,     0,    0,     0,    0] \n",
    "\n",
    "# ex1: 2の確率が最も高いと弾き出すモデルの出力の例\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t)) # 0.510825457099\n",
    "\n",
    "# ex2: 7の確率が最も高いと弾き出すモデルの出力の例\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t)) # 2.3025840929945458\n",
    "\n",
    "\n",
    "'''\n",
    "## ミニバッチ学習\n",
    "- 全ての訓練データに対して損失関数を求め、その値をできるだけ小さくするようなパラメータを探し出す必要がある\n",
    "- よって、先程までの一つの訓練データの損失関数を求める式の更に総和を求める\n",
    "    - E = -(1/N) * ∑(∑(tnk * logynk)) ... n, kは添字\n",
    "- しかし、MNISTのデータセットであっても60000万件あり、bigdataにもなればかなりのデータ量になる\n",
    "    - 全てのデータを対象に損失関数を計算するのは非現実的\n",
    "- そこで、訓練データからある枚数だけを選び出し、その塊ごとに学習を行なう\n",
    "    - この選びだしたものをミニバッチ = 小さな塊という\n",
    "    - [ ] 選び出す数 = batch_sizeの適切な決め方は？\n",
    "'''\n",
    "\n",
    "import sys, os\n",
    "sys.append.path('./deep-learning-from-scratch')\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_labeｌ=True)\n",
    "\n",
    "train_size = x_train.shape[0] # 60000\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "# ex: array([25286, 10861, 55404, 16791, 26595, 34692, 39286,  9747, 15529, 18955])\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        # reshape: n * m行列を、n' * m'行列に変形してくれる。ただし、要素数がおなじになるような変形に限る\n",
    "        # つまりここでは1次元配列を1 * nの2次元行列に変更している [1,2] => [[1. 2]]\n",
    "        t = t.reshape(1, t.size) \n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "        batch_size = y.shape[0] \n",
    "        return -np.sum(t * np.log(y)) / batch_size\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
