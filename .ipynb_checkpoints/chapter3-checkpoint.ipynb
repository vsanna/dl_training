{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5 11 17]\n",
      "[ 0.326  0.712]\n",
      "Accracy: 0.9352\n",
      "(10000, 784) (784, 50) (50, 100) (100, 10)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# 三章 ニューラルネットワーク\n",
    "- 適切な重みパラメータをデータから自動で学習できるというのがニューラルネットワーク(NN)の性質の一つ\n",
    "- ニューラルネットワークの概要図(P40)\n",
    "    - 入力層 -> 中間層（隠れ層） -> 出力層\n",
    "    - 構造だけみるとパーセプトロンと変わらない. \n",
    "- 一旦ここでパーセプトロンの復習と別表現方法を導入\n",
    "    - 入出力を関数として表現する。\n",
    "    - y = h(b + w1x1 + w2x2)\n",
    "    - この時のhを活性化関数と呼ぶことにする\n",
    "        - hは発火する閾値をコントロールしている\n",
    "    - この時、従来のしきは更に2つに分割できる\n",
    "        - a = b + w1x1 + w2x2\n",
    "- 二章で見たような、とある閾値を境に発火するか否かを切り替える関数をステップ関数、階段関数と呼ぶ\n",
    "    - よって、パーセプトロンでは活性化関数にステップ関数を利用しているといえることができる\n",
    "- 活性化関数にステップ関数以外を導入することで、NNの世界にすすめる!\n",
    "\n",
    "## シグモイド関数\n",
    "- NNでよく用いられている活性化関数の一つ\n",
    "    - h(x) = 1 / (1 + exp(-x))\n",
    "- パーセプトロンとNNの違いは活性化関数の違いくらい。多層につながる構造や信号の伝達方法は同じ\n",
    "'''\n",
    "\n",
    "# step関数の実装\n",
    "def step_function(x):\n",
    "    return 1 if x > 0 else 0\n",
    "\n",
    "# step関数をNumPyArrayを引数にもてるように変更\n",
    "def step_function(x):\n",
    "    y = x > 0\n",
    "    return y.astype(np.int)\n",
    "\n",
    "\n",
    "'''\n",
    "- [x] why numpyの配列を引数に持つような実装にする必要があるのか...\n",
    "    - 一括してステップ関数に複数の入力値を渡していた\n",
    "- np.ndarrayに対して不等号を与えると、map(lambda item:item > 0, x)したのと同じ結果をもつ配列が返される\n",
    "    - np.ndarrayはNumPyで最も重要なクラス。 N-d Array すなわち，N次元配列を扱うためのクラス\n",
    "- npのastypeメソッドは型変換を行なう\n",
    "    - booleanをintに変換すると、trueが1に、falseが0に変換される\n",
    "- つまり、この関数は複数の入力値を一括でステップ関数に渡す処理を行っている\n",
    "\n",
    "## step関数のグラフ\n",
    "- グラフ化にはmatplotlibを利用する\n",
    "- np.array(np.ndarray, dtype=None)\n",
    "    - 引数のnp.ndarray or arrayをnp.ndarrayに変換する。\n",
    "    代に引数にdtypeを指定するとそれに変換してくれる\n",
    "'''\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "def step_function(x):\n",
    "    return np.array(x > 0, dtype=np.int) # 前述と同じ実装\n",
    "\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1) # -5.0 ~ 5.0までの区間を0.1刻みで配列生成\n",
    "y = step_function(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "ｐlt.ylim(-0.1, 1.1) # y軸の範囲を指定\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "'''\n",
    "## シグモイド関数の実装\n",
    "'''\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "    # このxにnp.ndarrayを放り込んでも適切にmapで処理してくれる\n",
    "    # numpyのブロードキャストという機能による\n",
    "    # np.expがnp.ndarray\n",
    "\n",
    "\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "ｐlt.ylim(-0.1, 1.1) # y軸の範囲を指定\n",
    "# plt.pause(1)\n",
    "\n",
    "'''\n",
    "- 相違点\n",
    "    - グラフの滑らかさ\n",
    "    - つまりsigmoidは0/1ではなく、0-1の間の実数を返す\n",
    "- 共通点\n",
    "    - 出力を0-1に押し込める\n",
    "    - 入力が大きくなるに連れて出力も大きくなる\n",
    "    - ともに非線形関数\n",
    "- NNでは、活性化関数に**非線形関数を用いる必要がある**\n",
    "    - 線形関数をもちいると、線形関数ではNNの層を深くすることが出来ないため\n",
    "- その理由の簡単な説明\n",
    "    - h(x) = cx(c: 定数)とした時、y(x) = h(h(h(x)))と3層にしたとしても、それはh2(x) = c^3 * xと同義になってしまう\n",
    "    - つまり、多層にする利点を活かすことが出来ない\n",
    "    - [ ] 線形の活性化関数だと多層にしても1層の場合と同じになってしまうということは分かったが、多層の利点がわからないのでいまいちピンとこない\n",
    "\n",
    "## ReLU関数\n",
    "- sigmoid関数は古くからNNにおいて用いられてきた\n",
    "- 最近ではRectified Linear Unit関数が主に用いられる\n",
    "    - 入力が0を超えていれば、その入力をそのまま出力する\n",
    "'''\n",
    "\n",
    "def refu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "'''\n",
    "## 多次元配列の計算\n",
    "- 一旦ここで、np.ndarrayの配列計算を学ぶ\n",
    "'''\n",
    "\n",
    "A = np.array([1, 2, 3, 4])\n",
    "# print(A)        # [1, 2, 3, 4]\n",
    "np.ndim(A)  # 1次元\n",
    "A.shape       # (4, ) ... 1次元目の要素数が4つという意味\n",
    "A.shape[0] # 4\n",
    "\n",
    "B = np.array([[1,2], [3, 4], [5, 6]])\n",
    "# print(B)        \n",
    "np.ndim(B)  # 2次元\n",
    "B.shape       # (3, 2) ... 3 * 2の行列\n",
    "\n",
    "# 行列の内積 ... いわゆる行列の席を返す\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "np.dot(A, B) # A・B\n",
    "'''\n",
    "array([[19, 22],\n",
    "            [43, 50]])\n",
    "'''\n",
    "\n",
    "# これとは別\n",
    "A * B\n",
    "'''\n",
    "array([[ 5, 12],\n",
    "           [21, 32]])\n",
    "'''\n",
    "\n",
    "'''\n",
    "## NNの実装\n",
    "- まず、バイアスと活性化関数は無視し、重みだけがあるとするものを実装する場合。\n",
    "'''\n",
    "\n",
    "X = np.array([1, 2])\n",
    "W = np.array([[1, 3, 5], [2, 4, 6]])\n",
    "Y = np.dot(X, W)\n",
    "print(Y) # [ 5 11 17]\n",
    "\n",
    "'''\n",
    "- 実戦的なNNの実装\n",
    "    - 3層のNNを実装してみる\n",
    "        - ニューロンの数: 2 -> 3 -> 2 -> 2のようなモデル\n",
    "    - P60\n",
    "- 2層目から出力層への活性化関数だけを変えている\n",
    "    - 入力をそのまま出力する関数を恒等関数という\n",
    "    - [ ] why 恒等関数を利用するのか\n",
    "    - 出力層で利用する活性化関数は、解く問題の性質によって変える\n",
    "        - 回帰問題 => 恒等関数\n",
    "        - 2クラス分類 => シグモイド関数\n",
    "        - 他クラス分類 => ソフトマックス関数\n",
    "'''\n",
    "\n",
    "# 0層目から生まれた1層目を表現すると、\n",
    "# A1 = XW1 + B1となるので、\n",
    "\n",
    "X = np.array([1.0, 0.5])\n",
    "W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "B1 = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "A1 = np.dot(X, W1) + B1\n",
    "\n",
    "# A1を活性化関数にとうしたものをZ1とする\n",
    "Z1 = sigmoid(A1)\n",
    "\n",
    "# 2層目の表現\n",
    "W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "B2 = np.array([0.1, 0.2])\n",
    "A2 = np.dot(A1, W2) + B2\n",
    "Z2 = sigmoid(A2)\n",
    "\n",
    "\n",
    "# 2層目から出力層への活性化関数だけを変える\n",
    "\n",
    "def identify_function(x):\n",
    "    return x\n",
    "\n",
    "W3 = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "B3 = np.array([0.1, 0.2])\n",
    "\n",
    "A3 = np.dot(A2, W3)\n",
    "Y = identify_function(A3)\n",
    "print(Y)\n",
    "\n",
    "\n",
    "\n",
    "# ここまでを関数に落とし込むと...\n",
    "\n",
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['B1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['B2'] = np.array([0.1, 0.2])\n",
    "    network['B3'] = np.array([0.1, 0.2])\n",
    "    return network\n",
    "\n",
    "def pass_layer(x, w, b, active_func):\n",
    "    tmp = np.dot(x, w) + b\n",
    "    return active_func(tmp)\n",
    "\n",
    "\n",
    "def forward(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    B1, B2, B3 = network['B1'], network['B2'], network['B3']\n",
    "    a1 = pass_layer(x, W1, B1, sigmoid)\n",
    "    a2 = pass_layer(a1, W2, B2, sigmoid)\n",
    "    y = pass_layer(a2, W3, B3, identify_function)\n",
    "    return y\n",
    "    \n",
    "nw = init_network();\n",
    "x = np.array([1.0, 0.5])\n",
    "forward(nw, x) # array([ 0.31682708,  0.69627909])\n",
    "\n",
    "'''\n",
    "## 出力層の設計\n",
    "- NNは分類問題と回帰問題の両方に用いることができる\n",
    "- ただし、どちらに用いるかで出力層の活性化関数を変更する必要がある\n",
    "    - 回帰問題 => 恒等関数\n",
    "    - 分類問題 => ソフトマックス関数\n",
    "    - [ ] whyこういう使い分けが生まれる\n",
    "- なお、機械学習の問題は分類と回帰に大別できる\n",
    "- 恒等関数\n",
    "    - 入力と出力が一致する関数\n",
    "- ソフトマックス関数\n",
    "    - yk = exp(ak) / ∑(exp(ai)) ... i, kは添字\n",
    "    - 出力が全ての入力値から影響を受ける特質がある\n",
    "    - また、値が0-1に収まること、softmaxの返す配列の総和は常に1になることから、確率のようにみなすことができる\n",
    "    - [x] 何故にsoftmaxという名前なのか\n",
    "        -  x の各成分の中で xi がダントツで大きい → yi はほぼ 1 で y の他の成分はほぼ 0 になるような関数\n",
    "        - マックス関数（一番大きい成分を 11 にして，それ以外のものは 00 にする関数をこう呼ぶことにする）をソフトにしたという感じ\n",
    "        - [ソフトマックス関数 | 高校数学の美しい物語](http://mathtrain.jp/softmax)\n",
    "'''\n",
    "\n",
    "a = np.array([0.3, 2.9, 4.0])\n",
    "exp_a = np.exp(a) # array\n",
    "sum_exp_a = np.sum(exp_a)\n",
    "y = exp_a / sum_exp_a\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a) # array\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    return exp_a / sum_exp_a\n",
    "\n",
    "'''\n",
    "- しかし、このmethodだとaのいち要素が大きい時、例えば10000などの場合、e^10000を計算することになり、数が跳ね上がる\n",
    "- 結果、オーバーフローが起こりいる\n",
    "- そこで、このメソッドを次の特性を用いて修正する\n",
    "    - exp(ak) / ∑(exp(ai)) = (exp(ak) + C) / ∑(exp(ai) + C) ... p69\n",
    "    - Cに入力信号の中で最もおおきな値を用いることで数の膨れ上がりをある程度抑えることができる\n",
    "'''\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    return exp_a / sum_exp_a\n",
    "\n",
    "'''\n",
    "- softmax関数では、値が0-1に収まること、softmaxの返す配列の総和は常に1になることから、確率のようにみなすことができる\n",
    "- 出力内容を確率とみなすと、[0.018, 0.245, 0.736]は「74%の確率で二番目のクラスである」と解釈することができる\n",
    "- NNのクラス分類では、一般的に出力の一番大きいニューロンに相当するクラスだけを認識結果とする\n",
    "- また、softmaxでは、入力と出力の序列関係を変えることはないので、NNで分類をする際、出力層のソフトマックス層を省略する事ができる\n",
    "- [ ] なんとなく、softmaxは特徴を際立たせるための活性化関数のように見えるが、その解釈でいいのだろうか\n",
    "\n",
    "## 出力層のニューロンの数\n",
    "- 出力層のニューロンの数はとくべき問題に応じて適宜決める\n",
    "- クラス分類を行なう問題では、例えば数字を0-9のどれか判定する場合であれば10クラス分類なのでニューロンも10個用意する必要がある\n",
    "\n",
    "## 手書き数字認識\n",
    "- ここでは一旦「学習」は終わっているとして、学習済みのパラメータを使って、推論処理だけを行なう\n",
    "    - [ ] この推論処理をNNの順方向伝播と言うらしいが、なにを「この」と指しているかわからない\n",
    "- MNISTデータセット\n",
    "    - 機械学習の分野で最も有名なデータセットの一つ. よく使われる\n",
    "    - 訓練画像が6万枚, テスト画像が1万枚用意されている\n",
    "    - 一つ一つは28 * 28のグレー画像で、書くピクセルは0 - 255の値を取る\n",
    "- NNの推論処理を実装する\n",
    "    - 28 * 28のピクセルなので、入力層は784個のニューロンを持ち、出力層は10個のニューロンを持つ\n",
    "'''\n",
    "\n",
    "# メモがある場所からoreillyのrepoを見えるようにする\n",
    "import sys\n",
    "sys.path.append('deep-learning-from-scratch')\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "import pickle\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_labeｌ=False)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    # pythonにはpickleという、プログラmの実行中オブジェクトをファイルとして保存する機能がある\n",
    "    # rubyでいうserializeしたobjectみたいなもの？\n",
    "    with open('./deep-learning-from-scratch/ch03/sample_weight.pkl', 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "    '''\n",
    "    with open(\"...\") as f:\n",
    "        print(f.read())\n",
    "    は、以下と同じ意味。indent抜けた際に自動的にcloseしてくれる\n",
    "    f = open(\"...\")\n",
    "    print(f.read())\n",
    "    f.close()\n",
    "    '''    \n",
    "        \n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    B1, B2, B3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    z1 = pass_layer(x, W1, B1, sigmoid)\n",
    "    z2 = pass_layer(z1, W2, B2, sigmoid)\n",
    "    y = pass_layer(z2, W3, B3, softmax)\n",
    "    return y\n",
    "\n",
    "x, t = get_data()\n",
    "nw = init_network()\n",
    "\n",
    "accracy_cnt = 0\n",
    "for i in range(len(x)):\n",
    "    y = predict(nw, x[i])\n",
    "    p = np.argmax(y) # yの中で最大の要素 = 確率が最も高いもののindexを取得\n",
    "    if p == t[i]:\n",
    "        accracy_cnt += 1\n",
    "        \n",
    "print('Accracy: ' + str(float(accracy_cnt / len(x))))\n",
    "# Accracy: 0.9352\n",
    "\n",
    "'''\n",
    "- load_mnist関数でnormalize=Trueにすると、画像の各ピクセルを255で割り、データの値が0 - 1に収まるように変換する\n",
    "- このようなデータを歩き待った範囲に変換する処理を正規化という\n",
    "- NNの入力データに対して、何らかの決まった変換を行なうことを前処理という\n",
    "    - 識別性能の向上やスピードアップのために行なう\n",
    "    - 通常は標準偏差と平均をもちいて0を中心に分布するような正規化を行なうことがおおい\n",
    "'''\n",
    "\n",
    "W1, W2, W3 = nw['W1'],  nw['W2'], nw['W3']\n",
    "print(x.shape, W1.shape, W2.shape, W3.shape)\n",
    "# (10000, 784) (784, 50) (50, 100) (100, 10)  -> 10 => 形状を確認。行列の内積を行える形になっている\n",
    "\n",
    "'''\n",
    "- 画像のバッチ(まとめて)処理をするにはどうするか\n",
    "    - pythonの工夫を普通に行なう\n",
    "- [ ] (28 * 28)個のニューロンからなる入力内容が最終的に0-9のintに収まっていく過程で、隠れそうに当たる層は一体何をしているのか\n",
    "    - 正規化をしているので、1つのピクセルの実体は0-1の間に収まる実数である\n",
    "    - それに重みをつけて50個のニューロンにしたり、100個のニューロンにしたりしている\n",
    "    - その変換の過程でなぜ分類がされていくのか？\n",
    "    - たとえば、中心点にあたるピクセルの入力値が1 = 黒だったとして、最終的に1につながる重み付けならwが大きく、0に繋がる重み付けならwが小さくなるということ？\n",
    "'''\n",
    "\n",
    "x, t = get_data()\n",
    "nw = init_network()\n",
    "\n",
    "batch_size = 100\n",
    "accracy_cnt = 0\n",
    "\n",
    "for i in range(0, len(x), batch_size): # 第三引数でstepを設定できる\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    y_batch = predict(nw, x_ batch) # いま、predictはブロードキャストに対応している\n",
    "    p = np.argmax(y_batch, axis=1) # 1次元目を基準に最大要素のindexを返す\n",
    "    accracy_cnt += np.sum(p == t[i:i+batch_size]) \n",
    "    # np.ndarray同時の比較は、同じ位置の要素同士を比較したTrue/False配列を返す\n",
    "    # さらにnp.sumでtrueとなるものだけを数えている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
