{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6章 学習のテクニック\n",
    "- 学習において重要なテクニックが3つある\n",
    "    1. パラメータの更新方法\n",
    "        - これまでには、 x := x -λ(δL/δx)を使ってきた\n",
    "        - 他に、momentum, adagrad, adamなどがある\n",
    "    2. 初期パラメーターの設定\n",
    "        - 0はだめ\n",
    "        - 勾配消失\n",
    "        - 隠れ層のアクティベーションん分布が正規化する\n",
    "    3. batch normalization\n",
    "        - 矯正t系にアクティベーションの分布を調整する方法\n",
    "    4. 正則化\n",
    "    5. ハイパーパラメータの検証\n",
    "        \n",
    "## パラメータの更新\n",
    "- NNにおける学習の目的はLossFunctionをなるべく小さくするパラメーターを発見すること = 最適化\n",
    "- これまでは、SGD: 確率的勾配降下法を使ってきたが、他にもある\n",
    "- SGDの欠点は、関数の形状が等方的でない, 即ち谷のような形の場合非効率な経路で探索をしてしまうこと\n",
    "- それを解消する方法として３つある\n",
    "    1. Momentum\n",
    "    2. AdaGrad\n",
    "    3. Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# W := W - λ(δL/δW)で更新する\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "- momentum\n",
    "    - 新たにv: 速度を導入\n",
    "        - v := αv - λ(δL/δx)\n",
    "                - α: 0.9など固定値を用いる\n",
    "        - W := W + v\n",
    "- adagrad\n",
    "    - NNでは学習係数が小さすぎると学習に時間がかかりすぎるし、大きすぎると発散してしまう\n",
    "    - そこでlrを少しずつ小さくする学習係数の減衰(decay)という方法がある\n",
    "    - adagradはパラメーターの要素ごとに適応的に学習係数を調整しながら学習する\n",
    "        - h := h + (δL／δW) × (δL／δW) ... 要素ごとの掛け算\n",
    "        - W := W - λ * h ** (-1/2) * (δL／δW)\n",
    "    - 大きく更新された = よく動いたパラメーターの学習係数は次第に小さくなる\n",
    "- adam\n",
    "    - 詳細は論文に譲るが、効果的    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 重みの初期値\n",
    "- 初期値が0, というより重みを均一にしてしまうと、誤差逆伝播法においては全ての重みが均一に更新されてしまい意味がない\n",
    "- 勾配消失問題\n",
    "    - 逆伝播での勾配の値が小さくなり、消える問題\n",
    "    - 勾配が消失すると学習ができない(移動しなくなる)\n",
    "    - sigmoidなどを利用していて、アクティベーションの出力が0,1に偏るとsigmoidの傾きがほぼゼロになり生じることがアアル\n",
    "    - また、アクティベーション層の出力に偏りが無く、似たような出力をするとなると、それは複数のニューロンを保つ意味がなくなる = 表現力の制限\n",
    "    - **アクティベーション層の出力は適度な広がりを持つことを求められる**\n",
    "- 推奨される置き方1: Xavierの初期値\n",
    "    - 各層のアクティベーション を同じ広がりのある分布にしたい\n",
    "    - 前奏のノードの個数をnとした場合、n ** (-1/2)の標準偏差を持つ分布を使う\n",
    "    - w = no.random.randn(node_num, node_num) / np.sqrt(node_num)\n",
    "- 推奨される置き方2: Heの初期値\n",
    "    - アクティベーション関数がReLU関数の場合に適した置き方\n",
    "    - (2/n) ** (1/2)の標準偏差を持つ分布を使う\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Batch Normalization\n",
    "- Batch Normalizationとは\n",
    "    - 強制的にアクティベーションの分布を調整する\n",
    "- メリット\n",
    "    1. 学習を早く進行させられる\n",
    "    2. 初期値にそれほど依存しない = 神経質にならなくていい\n",
    "    3. 科学集を抑制する = Dropoutの必要性を減らす\n",
    "- Affine -> BatchNorm -> Activationとして、間に挟む\n",
    "- ミニバッチの単位で成果を行なう\n",
    "    - B = {x1, x2, ...} というm個の入力データがある\n",
    "    - xiを正規化(略)したBrを用意する\n",
    "    - ソレに対して固有のスケールとシフトで変換を行なう\n",
    "        - yi = γxi + β ... ここでガンマとベータはパラメタ. γ=1.0, β = 0.0からスタートし、学習によって適した値に調整する？\n",
    "        - [ ] 学習するパラメータをwから変えるということ？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 正則化\n",
    "- 過学習の問題\n",
    "    - 訓練データに特化しすぎて、汎化性能が低くなること\n",
    "    - trainとtestでaccracyの向上度合いにおおきな乖離が生まれているとき、それは汎化能力が低いことを示す\n",
    "- その原因\n",
    "    1. パラメーターを大量に持ち、表現力の高いモデルであること\n",
    "    2. 訓練データが少ないこと\n",
    "- 対処1: Weight Decay(荷重減衰)\n",
    "    - 学習の過程において、おおきな重みを持つことにたいしてペナルティを課す\n",
    "    - 損失関数にたいして、1/2λW**2を加える => 重みwが大きいほど損失関数が大きくなる => 重みを減らそうと動く\n",
    "        - [ ] L1ノルム, L2ノルム, L∞ノルムというのが出てきた。突然。\n",
    "    - これは認識精度の乖離を小さくすることが目的であり、精度は当然ながら「下がる」\n",
    "- 対処2: Dropout\n",
    "    - ニューロンをランダムに消去しながら学習する手法\n",
    "    - 隠れ層のニューロンをランダムに選び出し、そのニューロンを消去する\n",
    "    - 訓練時にはデータが流れるたびにランダムに削除するニューロンを選ぶ\n",
    "    - テスト時には全てのニューロンの信号を伝達するが、訓練時に消去した割合を掛けて出力をだす\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, drouput_ratio=0.5):\n",
    "        self.dropout_ratio = 0.5\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flag:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "# *x.shapeは引数で配列を展開してる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ハイパーパラメーターの検証\n",
    "- 各層のニューロンの数やバッチサイズ、学習係数やweight decayなどなど\n",
    "- 訓練データの中から20%ほどを検証データとして分離し、利用する\n",
    "- HPの最適化においてはエポックを小さくして、1会の放火に要する時間を短縮するのが有効\n",
    "- 手順\n",
    "    1. 範囲を設定する ... 大体10**-3 - 10**3からまず選ぶ\n",
    "    2. ランダムにサンプリング\n",
    "    3. エポック小さくして学習し、認識精度評価\n",
    "    4. 2-3を100回程度繰り返し、その結果からHPの良い位置に当たりをつける"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
