# about this

ゼロから作るDeep Learningの学習メモをまとめ、また各章のサマリーをまとめる（予定）

## 二章 パーセプトロン
- パーセプトロンとは
    - 複数の信号を受取り、一つの信号を出力するもの
    - y = 1 if wx + b > 0 else 0
- パラメーターである、wとb(θ)をコンピューターが適切に設定する作業を学習という
    - w: 重みは入力信号の重要度を示し、b: バイアスは発火しやすさを示す
- 人間の仕事は、
    1. パーセプトロンの構造 = モデルを考え、
    2. 学習データをコンピューターに与えること
- パーセプトロンでは層を重ねることで、表現が柔軟になる

## 三章 ニューラルネットワーク
- 適切な重みパラメータをデータから自動で学習できるというのがニューラルネットワークの特徴
- 活性化関数の導入: x -> a -> yの2段階に今までの一層での処理を分離
    - パーセプトロン: ステップ関数(階段関数)
    - NN: 様々な非線形の関数
        - sigmoid, ReLU: 2クラス分類
            - y = 1/(1 + np.exp(-a))
            - y = np.maxmim(0, a)
        - 恒等関数: 回帰問題
            - y = a
        - softmax: 多クラス分類 ... ex: MNIST
            - yk = exp(ak) / ∑(exp(ai)) 
            - y = np.exp(a) / np.sum(np.exp(a))
- pythonへの習熟と、今後を見据えバッチ処理の工夫の仕方を学ぶ

## 四章 NNの学習
- ここまでの推測の種類の生理
    1. 入力 -> 人間の考えたアルゴリズム -> 出力
    2. 入力 -> 人間の考えた特徴量 -> 機械学習 -> 出力
    3. 入力 -> NN(deep learning) -> 出力
- 特徴量とは、入力データから、本質的なデータを的確に抽出できるように設計された「変換器」を指す
    - 画像の特徴量は一般的にベクトルで記述される
        - コンピュータビジョンで有名な特徴量としては、SIFT, SURF, HOGなどがある
    - 画像データを変換器　= 特徴量によってベクトルに変換し、そのベクトルに対して「識別器」で学習させる
        - SVN, KNN等があるらしい
- この「特徴量」については人間が設計する必要がある
    - 問題に応じて特徴量 = 変換器を使い分ける必要がある
    - ココには人の手が介在する
- 過学習とは
- one-hot表現
- 損失関数
    - 二乗和誤差
    - 交差エントロピー誤差
